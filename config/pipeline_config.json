{
    "retrievers": [
        {
            "model_name": "BiEncoderRetriever",
            "retriever_model": "BAAI/bge-multilingual-gemma2",
            "top_n": 400,
            "max_length": 4096,
            "task_description": "Retrieve the most relevant fact-checked claims that either support or counter the main statements in the given post. Prioritize high contextual similarity to the main claim, focusing on factual and subject alignment.",
            "save_predictions": "predictions_400_eng",
            "version": 1
        },
        {
            "model_name": "CrossEncoderRetriever",
            "retriever_model": "intfloat/multilingual-e5-large-instruct",
            "top_n": 400,
            "max_length": 4096,
            "task_description": "Retrieve the most relevant fact-checked claims that either support or counter the main statements in the given post. Prioritize high contextual similarity to the main claim, focusing on factual and subject alignment.",
            "save_predictions": "predictions_400_eng",
            "version": 1
        }
    ],
    "rerankers": [
        {
            "model_name": "CrossEncoderReranker",
            "reranker_model": "Alibaba-NLP/gte-Qwen2-7B-instruct",
            "max_length": 8192,
            "top_n": 50,
            "top_predictions": "retriever_aggregated/predictions_100.json",
            "task_description": "Given a claim, rerank the fact-checked claims that either support or reject the main statements in the given post. Prioritize high contextual similarity to the main claim, focusing on factual and subject alignment.",
            "save_predictions": "predictions",
            "version": 0
        },
        {
            "model_name": "CrossEncoderGritLMReranker",
            "reranker_model": "GritLM/GritLM-7B",
            "max_length": 8192,
            "top_n": 50,
            "top_predictions": "retriever_aggregated/predictions_100.json",
            "task_description": "Given a claim, rerank the fact-checked claims that either support or reject the main statements in the given post. Prioritize high contextual similarity to the main claim, focusing on factual and subject alignment.",
            "save_predictions": "predictions",
            "version": 0
        },
        {
            "model_name": "BiEncoderReranker",
            "reranker_model": "nvidia/NV-Embed-v2",
            "max_length": 2768,
            "top_n": 50,
            "top_predictions": "retriever_aggregated/predictions_100.json",
            "task_description": "Given a claim, rerank the fact-checked claims that either support or reject the main statements in the given post. Prioritize high contextual similarity to the main claim, focusing on factual and subject alignment.",
            "save_predictions": "predictions",
            "version": 0
        }
    ],
    "ensembler_mode": "retriever",
    "retriever_ensembler": 
        {
        "top_n": 150,
        "predictions_dirs": [
                            "predictions_retriever_intfloat/multilingual-e5-large-instruct/predictions_100_eng.json",
                            "predictions_bm25_top_400_eng/predictions.json",
                            "predictions_retriever_BAAI/bge-multilingual-gemma2/predictions_400_eng.json"
                        ],
        "output_dir": "retriever_aggregated",
        "output_file": "predictions_100"
        }
    ,
    "reranker_ensembler": 
        {
        "top_n": 10,
        "predictions_dirs": [
                            "aggregated_predictions_reranker_Alibaba-NLP/gte-Qwen2-7B-instruct/predictions_eng.json",
                            "aggregated_predictions_reranker_GritLM/GritLM-7B/predictions.json",
                            "aggregated_predictions_reranker_nvidia/NV-Embed-v2/predictions_eng.json"
                        ],
        "output_dir": "reranker_aggregated",
        "output_file": "predictions"
        }


}